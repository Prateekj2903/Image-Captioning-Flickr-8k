{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prateek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pickle import dump, load\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Directory: /media/prateek/New Volume1/Datasets/Flickr-8k/Flicker8k_Dataset\n",
      "Descriptions Directory: /media/prateek/New Volume1/Datasets/Flickr-8k/Descriptions/Flickr8k.token.txt\n"
     ]
    }
   ],
   "source": [
    "curr_path = os.getcwd()\n",
    "descriptions_file_dir = os.path.join(curr_path, \"Descriptions/Flickr8k.token.txt\")\n",
    "train_images_dir = os.path.join(curr_path, \"Descriptions/Flickr_8k.trainImages.txt\")\n",
    "test_images_dir = os.path.join(curr_path, \"Descriptions/Flickr_8k.testImages.txt\")\n",
    "dev_images_dir = os.path.join(curr_path, \"Descriptions/Flickr_8k.devImages.txt\")\n",
    "images_dir = os.path.join(curr_path, \"Flicker8k_Dataset\")\n",
    "\n",
    "save_dir = os.path.join(curr_path, \"preprocessed_data\")\n",
    "features_dict_path = os.path.join(save_dir, 'features_dict')\n",
    "descriptions_dict_path = os.path.join(save_dir, 'descriptions_dict')\n",
    "descriptions_text_path = os.path.join(save_dir, 'descriptions_text')\n",
    "\n",
    "print(\"Image Directory: {}\\nDescriptions Directory: {}\".format(images_dir, descriptions_file_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(doc_path):\n",
    "    file = open(doc_path, 'r')\n",
    "    all_text = file.read()\n",
    "    file.close()\n",
    "    return all_text\n",
    "\n",
    "def load_data(file_path):\n",
    "    return load(open(file_path, 'rb'))\n",
    "\n",
    "def load_image_ids(file_path):\n",
    "    data = load_document(file_path)\n",
    "    img_ids = []\n",
    "    for line in data.split('\\n'):\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        img_ids.append(line.split('.')[0])\n",
    "    return img_ids\n",
    "\n",
    "def load_train_image_ids():\n",
    "    return load_image_ids(train_images_dir)\n",
    "\n",
    "def load_dev_image_ids():\n",
    "    return load_image_ids(dev_images_dir)\n",
    "\n",
    "def load_image_descriptions(desc_dict_file_path, img_ids):\n",
    "    descriptions_dict = load_data(desc_dict_file_path)\n",
    "    desc_dict = dict()\n",
    "    for id in img_ids:\n",
    "        desc_dict[id] = descriptions_dict[id]\n",
    "    return desc_dict\n",
    "\n",
    "def load_train_image_descriptions(train_image_ids):\n",
    "    return load_image_descriptions(descriptions_dict_path, train_image_ids)\n",
    "\n",
    "def load_dev_image_descriptions(dev_image_ids):\n",
    "    return load_image_descriptions(descriptions_dict_path, dev_image_ids)\n",
    "\n",
    "def load_image_features(features_file_path, img_ids):\n",
    "    features_dict = load_data(features_file_path)\n",
    "    feat_dict = dict()\n",
    "    for id in img_ids:\n",
    "        feat_dict[id] = features_dict[id]\n",
    "    return feat_dict\n",
    "\n",
    "def load_train_image_features(train_image_ids):\n",
    "    return load_image_features(features_dict_path, train_image_ids)\n",
    "\n",
    "def load_dev_image_features(dev_image_ids):\n",
    "    return load_image_features(features_dict_path, dev_image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "6000\n",
      "6000\n",
      "\n",
      "1000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "train_image_ids = load_train_image_ids()\n",
    "train_descriptions_dict = load_train_image_descriptions(train_image_ids)\n",
    "train_features_dict = load_train_image_features(train_image_ids)\n",
    "\n",
    "dev_image_ids = load_dev_image_ids()\n",
    "dev_descriptions_dict = load_dev_image_descriptions(dev_image_ids)\n",
    "dev_features_dict = load_dev_image_features(dev_image_ids)\n",
    "\n",
    "print(len(train_image_ids))\n",
    "print(len(train_descriptions_dict))\n",
    "print(len(train_features_dict))\n",
    "print()\n",
    "print(len(dev_image_ids))\n",
    "print(len(dev_descriptions_dict))\n",
    "print(len(dev_features_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptions_to_list(descriptions_dict):\n",
    "    desc_list = list()\n",
    "    for key in descriptions_dict.keys():\n",
    "        for desc in descriptions_dict[key]:\n",
    "            desc_list.append(desc)\n",
    "    return desc_list\n",
    "\n",
    "def create_tokenizer(descriptions_list):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(descriptions_list)\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    max_length = max(len(desc.split()) for desc in descriptions_list)\n",
    "    return tokenizer, vocab_size, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions_list = descriptions_to_list(train_descriptions_dict)\n",
    "train_tokenizer, vocab_size, max_length = create_tokenizer(train_descriptions_list)\n",
    "# test_tokenizer, _, _ = create_tokenizer(test_img_desc)\n",
    "# X1, X2, Y = create_sequences(tokenizer, train_img_desc, train_img_features, vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(max_length, vocab_size):\n",
    "    input1 = Input(shape=(1024,))\n",
    "    x1 = Dropout(0.25)(input1)\n",
    "    x1 = Dense(units=256, activation='relu')(x1)\n",
    "    \n",
    "    input2 = Input(shape=(max_length,))\n",
    "    x2 = Embedding(vocab_size, 256, mask_zero=True)(input2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "    x2 = LSTM(256)(x2)\n",
    "    \n",
    "    d = add([x1, x2])\n",
    "    d = Dense(256, activation='relu')(d)\n",
    "    \n",
    "    output = Dense(units=vocab_size, activation='softmax')(d)\n",
    "    \n",
    "    model = Model(inputs=[input1, input2], outputs=output)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 36)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 36, 256)      1862656     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1024)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 36, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          262400      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          525312      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 7276)         1869932     dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,586,092\n",
      "Trainable params: 4,586,092\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = model(max_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(tokenizer, descriptions, features, vocab_size, max_length):\n",
    "    X1, X2, Y = np.zeros((1, 1024)), np.zeros((1, max_length)), np.zeros((1, vocab_size))\n",
    "\n",
    "    for j, desc_list in enumerate(descriptions):\n",
    "        for desc in desc_list:\n",
    "            encoded_seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "            for i in range(1, len(encoded_seq)):\n",
    "                inp_seq = encoded_seq[:i]\n",
    "                out_seq = encoded_seq[i]\n",
    "\n",
    "                inp_seq = pad_sequences([inp_seq], maxlen=max_length)[0]\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "                X1 = np.vstack((X1, features[j]))\n",
    "                X2 = np.vstack((X2, inp_seq))\n",
    "                Y = np.vstack((Y, out_seq))\n",
    "                \n",
    "    return X1[1:], X2[1:], Y[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(tokenizer, train_descriptions_dict, train_features_dict, vocab_size, max_length, steps_per_epoch):\n",
    "    batch_size = len(train_descriptions_dict) // steps_per_epoch\n",
    "    x1 = []\n",
    "    y1 = []\n",
    "    features = []\n",
    "    descriptions = []\n",
    "    i = 1\n",
    "    while 1:\n",
    "        for key, desc_list in train_descriptions_dict.items():\n",
    "            features.append(train_features_dict[key][0])\n",
    "            descriptions.append(desc_list)\n",
    "            \n",
    "            if i == batch_size:\n",
    "                input1, input2, output = create_sequences(tokenizer, descriptions, features, vocab_size, max_length)\n",
    "                features = []\n",
    "                descriptions = []\n",
    "                i = 1\n",
    "                yield([input1, input2], output)\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 6000\n",
    "generator = data_generator(train_tokenizer, train_descriptions_dict, train_features_dict, vocab_size, max_length, steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 1024)\n",
      "(60, 36)\n",
      "(60, 7276)\n"
     ]
    }
   ],
   "source": [
    "inputs, outputs = next(generator)\n",
    "print(inputs[0].shape)\n",
    "print(inputs[1].shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 276s 92ms/step - loss: 3.8052 - acc: 0.3714\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 230s 77ms/step - loss: 3.2589 - acc: 0.4240\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 234s 78ms/step - loss: 2.9479 - acc: 0.4386\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 250s 83ms/step - loss: 2.8384 - acc: 0.4514\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 248s 83ms/step - loss: 2.6904 - acc: 0.4577\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 247s 82ms/step - loss: 2.6419 - acc: 0.4654\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 249s 83ms/step - loss: 2.5328 - acc: 0.4690\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 246s 82ms/step - loss: 2.5172 - acc: 0.4755\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 244s 81ms/step - loss: 2.4208 - acc: 0.4782\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 242s 81ms/step - loss: 2.4294 - acc: 0.4828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe03d33b908>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "batch_size = 2\n",
    "steps_per_epoch = len(train_descriptions_dict) // batch_size\n",
    "\n",
    "generator = data_generator(train_tokenizer, train_descriptions_dict, train_features_dict, vocab_size, max_length, steps_per_epoch)\n",
    "model.fit_generator(generator, epochs=10, verbose=1, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.155919\n",
      "BLEU-2: 0.082435\n",
      "BLEU-3: 0.057821\n",
      "BLEU-4: 0.022804\n"
     ]
    }
   ],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "#     seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = np.argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    for key, desc_list in descriptions.items():\n",
    "    # generate description\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        # store actual and predicted\n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "\n",
    "# prepare tokenizer on train set\n",
    "\n",
    "# # load training dataset (6K)\n",
    "# filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "# train = load_set(filename)\n",
    "# print('Dataset: %d' % len(train))\n",
    "# # descriptions\n",
    "# train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "# print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# # prepare tokenizer\n",
    "# tokenizer = create_tokenizer(train_descriptions)\n",
    "# vocab_size = len(tokenizer.word_index) + 1\n",
    "# print('Vocabulary Size: %d' % vocab_size)\n",
    "# # determine the maximum sequence length\n",
    "# max_length = max_length(train_descriptions)\n",
    "# print('Description Length: %d' % max_length)\n",
    "\n",
    "# prepare test set\n",
    "\n",
    "# load test set\n",
    "# filename = 'Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "# test = load_set(filename)\n",
    "# print('Dataset: %d' % len(test))\n",
    "# # descriptions\n",
    "# test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "# print('Descriptions: test=%d' % len(test_descriptions))\n",
    "# # photo features\n",
    "# test_features = load_photo_features('features.pkl', test)\n",
    "# print('Photos: test=%d' % len(test_features))\n",
    "\n",
    "# # load the model\n",
    "# filename = 'model-ep002-loss3.245-val_loss3.612.h5'\n",
    "# model = load_model(filename)\n",
    "# evaluate model\n",
    "evaluate_model(model, dev_descriptions_dict, dev_features_dict, train_tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
