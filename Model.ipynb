{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\windo\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pickle import dump, load\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Directory: D:\\Datasets\\Flickr-8k\\Flicker8k_Dataset\n",
      "Descriptions Directory: D:\\Datasets\\Flickr-8k\\Descriptions/Flickr8k.token.txt\n"
     ]
    }
   ],
   "source": [
    "curr_path = os.getcwd()\n",
    "descriptions_file_dir = os.path.join(curr_path, \"Descriptions/Flickr8k.token.txt\")\n",
    "train_images_dir = os.path.join(curr_path, \"Descriptions/Flickr_8k.trainImages.txt\")\n",
    "test_images_dir = os.path.join(curr_path, \"Descriptions/Flickr_8k.testImages.txt\")\n",
    "dev_images_dir = os.path.join(curr_path, \"Descriptions/Flickr_8k.devImages.txt\")\n",
    "images_dir = os.path.join(curr_path, \"Flicker8k_Dataset\")\n",
    "\n",
    "save_dir = os.path.join(curr_path, \"preprocessed_data\")\n",
    "features_dict_path = os.path.join(save_dir, 'features_dict')\n",
    "descriptions_dict_path = os.path.join(save_dir, 'descriptions_dict')\n",
    "descriptions_text_path = os.path.join(save_dir, 'descriptions_text')\n",
    "\n",
    "print(\"Image Directory: {}\\nDescriptions Directory: {}\".format(images_dir, descriptions_file_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(doc_path):\n",
    "    file = open(doc_path, 'r')\n",
    "    all_text = file.read()\n",
    "    file.close()\n",
    "    return all_text\n",
    "\n",
    "def load_data(file_path):\n",
    "    return load(open(file_path, 'rb'))\n",
    "\n",
    "def load_image_ids(file_path):\n",
    "    data = load_document(file_path)\n",
    "    img_ids = []\n",
    "    for line in data.split('\\n'):\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        img_ids.append(line.split('.')[0])\n",
    "    return img_ids\n",
    "\n",
    "def load_train_image_ids():\n",
    "    return load_image_ids(train_images_dir)\n",
    "\n",
    "def load_dev_image_ids():\n",
    "    return load_image_ids(dev_images_dir)\n",
    "\n",
    "def load_image_descriptions(desc_dict_file_path, img_ids):\n",
    "    descriptions_dict = load_data(desc_dict_file_path)\n",
    "    desc_dict = dict()\n",
    "    for id in img_ids:\n",
    "        desc_dict[id] = descriptions_dict[id]\n",
    "    return desc_dict\n",
    "\n",
    "def load_train_image_descriptions(train_image_ids):\n",
    "    return load_image_descriptions(descriptions_dict_path, train_image_ids)\n",
    "\n",
    "def load_dev_image_descriptions(dev_image_ids):\n",
    "    return load_image_descriptions(descriptions_dict_path, dev_image_ids)\n",
    "\n",
    "def load_image_features(features_file_path, img_ids):\n",
    "    features_dict = load_data(features_file_path)\n",
    "    feat_dict = dict()\n",
    "    for id in img_ids:\n",
    "        feat_dict[id] = features_dict[id]\n",
    "    return feat_dict\n",
    "\n",
    "def load_train_image_features(train_image_ids):\n",
    "    return load_image_features(features_dict_path, train_image_ids)\n",
    "\n",
    "def load_dev_image_features(dev_image_ids):\n",
    "    return load_image_features(features_dict_path, dev_image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "6000\n",
      "6000\n",
      "\n",
      "1000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "train_image_ids = load_train_image_ids()\n",
    "train_descriptions_dict = load_train_image_descriptions(train_image_ids)\n",
    "train_features_dict = load_train_image_features(train_image_ids)\n",
    "\n",
    "dev_image_ids = load_dev_image_ids()\n",
    "dev_descriptions_dict = load_dev_image_descriptions(dev_image_ids)\n",
    "dev_features_dict = load_dev_image_features(dev_image_ids)\n",
    "\n",
    "print(len(train_image_ids))\n",
    "print(len(train_descriptions_dict))\n",
    "print(len(train_features_dict))\n",
    "print()\n",
    "print(len(dev_image_ids))\n",
    "print(len(dev_descriptions_dict))\n",
    "print(len(dev_features_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptions_to_list(descriptions_dict):\n",
    "    desc_list = list()\n",
    "    for key in descriptions_dict.keys():\n",
    "        for desc in descriptions_dict[key]:\n",
    "            desc_list.append(desc)\n",
    "    return desc_list\n",
    "\n",
    "def create_tokenizer(descriptions_list):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(descriptions_list)\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    max_length = max(len(desc.split()) for desc in descriptions_list)\n",
    "    return tokenizer, vocab_size, max_length\n",
    "    \n",
    "def create_sequences(tokenizer, desc_list, feature, vocab_size, max_length):\n",
    "    X1, X2, Y = list(), list(), list()\n",
    "    \n",
    "    for desc in desc_list:\n",
    "        encoded_seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        for i in range(1, len(encoded_seq)):\n",
    "            inp_seq = encoded_seq[:i]\n",
    "            out_seq = encoded_seq[i]\n",
    "\n",
    "            inp_seq = pad_sequences([inp_seq], maxlen=max_length)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "            X1.append(feature)\n",
    "            X2.append(inp_seq)\n",
    "            Y.append(out_seq)\n",
    "                \n",
    "    return np.array(X1), np.array(X2), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions_list = descriptions_to_list(train_descriptions_dict)\n",
    "train_tokenizer, vocab_size, max_length = create_tokenizer(train_descriptions_list)\n",
    "# test_tokenizer, _, _ = create_tokenizer(test_img_desc)\n",
    "# X1, X2, Y = create_sequences(tokenizer, train_img_desc, train_img_features, vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_descriptions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 1, 2, 16, 10, 8, 33, 254, 2, 15, 10, 5, 6, 43, 3, 1]]\n",
      "start_seq a black dog is running after a white dog in the snow end_seq\n"
     ]
    }
   ],
   "source": [
    "print(train_tokenizer.texts_to_sequences([train_descriptions_list[0]]))\n",
    "print(train_descriptions_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 1, 2, 26, 10, 5, 6, 43, 108, 174, 868, 89, 5, 76, 68, 3, 1]]\n",
      "start_seq a brown dog in the snow has something hot pink in its mouth end_seq\n"
     ]
    }
   ],
   "source": [
    "print(train_tokenizer.texts_to_sequences([train_descriptions_list[10]]))\n",
    "print(train_descriptions_list[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n",
      "1\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "\n",
      "[4, 1]\n",
      "2\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 1]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "\n",
      "[4, 1, 2]\n",
      "16\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 1 2]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "[4, 1, 2, 16]\n",
      "10\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  4  1  2 16]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "[4, 1, 2, 16, 10]\n",
      "8\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  4  1  2 16 10]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "[4, 1, 2, 16, 10, 8]\n",
      "33\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  4  1  2 16 10  8]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "[4, 1, 2, 16, 10, 8, 33]\n",
      "254\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  4  1  2 16 10  8 33]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "[4, 1, 2, 16, 10, 8, 33, 254]\n",
      "2\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   4   1   2  16  10   8  33 254]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "\n",
      "[4, 1, 2, 16, 10, 8, 33, 254, 2]\n",
      "15\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   4   1   2  16  10   8  33 254   2]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "[4, 1, 2, 16, 10, 8, 33, 254, 2, 15]\n",
      "10\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   4   1   2  16  10   8  33 254   2  15]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "[4, 1, 2, 16, 10, 8, 33, 254, 2, 15, 10]\n",
      "5\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   4   1   2  16  10   8  33 254   2  15  10]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "[4, 1, 2, 16, 10, 8, 33, 254, 2, 15, 10, 5]\n",
      "6\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   4   1   2  16  10   8  33 254   2  15  10   5]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "[4, 1, 2, 16, 10, 8, 33, 254, 2, 15, 10, 5, 6]\n",
      "43\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   4   1   2  16  10   8  33 254   2  15  10   5   6]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "[4, 1, 2, 16, 10, 8, 33, 254, 2, 15, 10, 5, 6, 43]\n",
      "3\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   4   1   2  16  10   8  33 254   2  15  10   5   6  43]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "[4, 1, 2, 16, 10, 8, 33, 254, 2, 15, 10, 5, 6, 43, 3]\n",
      "1\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   4   1   2  16  10   8  33 254   2  15  10   5   6  43   3]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_seq = train_tokenizer.texts_to_sequences([train_descriptions_list[0]])[0]\n",
    "for i in range(1, len(encoded_seq)):\n",
    "    inp_seq = encoded_seq[:i]\n",
    "    out_seq = encoded_seq[i]\n",
    "    \n",
    "    print(inp_seq)\n",
    "    print(out_seq)\n",
    "    inp_seq = pad_sequences([inp_seq], maxlen=max_length)[0]\n",
    "    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "    print(inp_seq)\n",
    "    print(out_seq)\n",
    "    print()\n",
    "\n",
    "#     inp_seq = pad_sequences([inp_seq], maxlen=max_length)[0]\n",
    "#     out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "#     X1.append(feature)\n",
    "#     X2.append(inp_seq)\n",
    "#     Y.append(out_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
